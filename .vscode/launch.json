{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "autotestia test",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": ["test"],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia generate from pptx",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [
                "generate",
                "C:/Users/Oscar/My Drive (oscarj1994@gmail.com)/9. DOCENCIA/2024-2025/2nd sem - Procesamiento del Lenguaje Natural/Tema 5b.pptx",
                "-o", "generated/tema5b_new.md",
                "-n", "4",
                "--provider", "anthropic",
                "--generator-model", "claude-sonnet-4-5",
                "--reviewer-model", "claude-sonnet-4-5",
                "--evaluator-model", "claude-sonnet-4-5",
                "--evaluate-initial",
                "--evaluate-reviewed",
                "--use-llm-review",
                "--generator-instructions", "Create questions about the specifics of LDA, LSA / LSI",
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia generate from instructions",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [
                "generate",
                "--generator-instructions", "Generate multiple-choice questions specifically about Python regular expressions using the 're' module. Focus on questions requiring either synthesis (writing a regex pattern based on a description) or analysis (determining what a given regex pattern matches or does). Cover common concepts like character classes, quantifiers, grouping, anchors, lookarounds, and standard 're' functions (e.g., search, match, findall, sub).",
                "-n", "6",
                "--output-dir", "generated/python_regex_questions_2",
                "--provider", "anthropic",
                "--use-llm-review",
                "--reviewer-instructions", "Ensure questions accurately test Python regex synthesis or analysis as requested. Verify the correctness of regex patterns, expected matches, and explanations. Ensure distractors are plausible but incorrect applications or interpretations of regex concepts.",
                "--language", "Spanish",
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia generate from md",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [
                "generate",
                "C:\\Users\\Oscar\\My Drive (oscarj1994@gmail.com)\\9. DOCENCIA\\2025-2026\\1er sem - MIB - Sistemas informáticos y sistemas de información\\slides_2025\\xml.md",
                "-o", "generated/mib/xml_gemini25proflash_latest.md",
                "-n", "4",
                "--generator-model", "google/gemini-2.5-pro",
                "--reviewer-model", "google/gemini-2.5-flash", 
                "--evaluator-model", "google/gemini-2.5-flash",
                "--use-llm-review",
                "--language", "es",
                "--generator-instructions", "Include some exercises, both for synthesis and analysis",
                "--evaluate-initial",
                "--evaluate-reviewed",
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia split",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "console": "integratedTerminal",
            "justMyCode": true,
            "cwd": "${workspaceFolder}",
            "args": [
                "split",
                "generated/mib/xml_gemini25proflash_latest.md",
                "--splits", "5", "5", "-1",
                "--output-dir", "generated/mib/",
                "--shuffle", "123",
                "--log-level", "DEBUG"
            ]
        },
        {
            "name": "autotestia merge",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [   
                "merge",
                "generated/mib/xml_gemini25proflash_latest_1.md",
                "generated/mib/xml_gemini25proflash_latest_2.md",
                "generated/mib/xml_gemini25proflash_latest_3.md",
                "-o", "generated/mib/xml_gemini25proflash_latest_all.md",
                "--log-level", "DEBUG"
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia shuffle",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [
                "shuffle",
                "generated/mib/xml_gemini25proflash_latest_all.md",
                "--seed", "42",
                "--yes",
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia export to Wooclap",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [
                "export",
                "wooclap",
                "generated/mib/xml_gemini25proflash_latest_all.md",
                "--shuffle-questions", "123",
                "--shuffle-answers", "123",
                "--evaluate-final"
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia export to R/Exams",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [
                "export",
                "rexams",
                "generated/mib/xml_gemini25proflash_latest.md",
                "--exam-title", "Sistemas informáticos y sistemas de información - Examen Parcial",
                "--exam-course", "Máster en Ingeniería Biomédica",
                "--shuffle-answers", "123",
                "--exam-date", "2025-10-22",
                "--evaluate-final",
                "--log-level", "DEBUG"
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia export to pexams",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [
                "export",
                "pexams",
                "generated/mib/xml_gemini25proflash_latest.md",
                // "generated_test/merged_questions.md",
                "--exam-title", "Sistemas informáticos y sistemas de información - Examen Parcial",
                "--exam-course", "Máster en Ingeniería Biomédica",
                "--shuffle-answers", "123",
                "--exam-date", "2025-10-22",
                "--exam-models", "1",
                "--exam-font-size", "10pt",
                "--exam-language", "es",
                "--exam-generate-fakes", "1",
                "--exam-generate-references",
                "--exam-columns", "2",
                "--max-image-height", "300",
                "--log-level", "DEBUG"
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia correct pexams",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "cwd": "${workspaceFolder}",
            "args": [
                "correct",
                "pexams",
                "--input-path",
                "generated/mib/xml_gemini25proflash_latest_all_pexams_output/simulated_scans",
                "--exam-dir",
                "generated/mib/xml_gemini25proflash_latest_all_pexams_output",
                "--output-dir",
                "generated/mib/xml_gemini25proflash_latest_all_pexams_output/correction_results",
                "--log-level", "DEBUG"
            ],
            "console": "integratedTerminal",
            "justMyCode": true
        },
        {
            "name": "autotestia correct rexams",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/main.py",
            "args": [
                "correct", "rexams",
                "--all-scans-pdf", "./generated/mib/xml_gemini25proflash_latest_all_rexams_pdf_output/20251022182231907.pdf",
                "--student-info-csv", "generated/mib/xml_gemini25proflash_latest_all_rexams_pdf_output/data.csv",
                "--solutions-rds", "generated/mib/xml_gemini25proflash_latest_all_rexams_pdf_output/exam.rds",
                "--output-path", "generated/mib/xml_gemini25proflash_latest_all_rexams_corrected",
                "--exam-language", "es",
                "--partial-eval",
                "--negative-points", "-0.333333",
                "--scale-mark-to", "10.0",
                "--student-csv-id-col", "DNI",
                "--student-csv-reg-col", "DNI",
                "--student-csv-name-col", "Nom",
                "--student-csv-surname-col", "Cognom",
                "--student-csv-encoding", "UTF-8",
                "--registration-format", "%08s",
                "--python-rotate-control",
                "--split-pages-python-control",
                "--max-score", "32",
                "--log-level", "INFO",
                "--python-bw-threshold", "100",
                "--void-questions-nicely", "14",
            ],
            "console": "integratedTerminal",
            "justMyCode": true,
            "cwd": "${workspaceFolder}"
        },
    ]
} 